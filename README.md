# ü§ñ AI Interviewer - Multi-Agent Interview Simulation Platform

A realistic AI-powered interview simulation system built with **LangGraph** and **Azure OpenAI**. This project demonstrates multi-agent orchestration where three specialized AI agents (Technical, HR, and Manager) conduct a comprehensive interview, followed by an AI-powered evaluation.

## üåü Features

### Multi-Agent Interview System
- **üíª Technical Agent (Alex)**: Asks 6 technical questions focusing on coding, system design, and problem-solving
- **ü§ù HR Agent (Olivia)**: Asks 3 questions about cultural fit, soft skills, and teamwork
- **üëî Manager Agent (Rahul)**: Asks 2 questions about leadership, strategy, and career vision
- **üéØ Evaluation Agent**: Provides comprehensive AI-generated feedback with scores, strengths, weaknesses, and suggestions

### Key Capabilities
- **Experience-Level Adaptive**: Questions tailored to Junior, Mid-Level, or Senior positions
- **Resume Upload Support**: Upload PDF, DOCX, or TXT resumes for personalized questions
- **LangGraph Orchestration**: Sophisticated workflow management with state transitions
- **Azure OpenAI Integration**: Leverages Azure OpenAI models for intelligent question generation
- **Context-Aware**: Agents adapt questions based on previous answers, candidate profile, and resume
- **Professional Web UI**: Beautiful Streamlit interface with real-time progress tracking
- **AI-Powered Evaluation**: Intelligent analysis of interview performance across all rounds
- **Comprehensive Logging**: Detailed logging system for debugging and monitoring
- **Modular Architecture**: Clean, maintainable code following industry best practices

## üìÅ Project Structure

```
AI_Interviewer/
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ config.toml              # Streamlit auto-reload configuration
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py              # Environment variables and configuration
‚îÇ   ‚îî‚îÄ‚îÄ logging_config.py        # Logging setup with file and console handlers
‚îú‚îÄ‚îÄ logs/                        # Application logs (auto-generated, not in git)
‚îÇ   ‚îî‚îÄ‚îÄ ai_interviewer_*.log     # Timestamped log files
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py        # Base agent class with LLM initialization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ technical_agent.py   # Alex - Technical interviewer (6 questions)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hr_agent.py          # Olivia - HR interviewer (3 questions)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manager_agent.py     # Rahul - Hiring manager (2 questions)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation_agent.py  # AI evaluation specialist with scoring
‚îÇ   ‚îú‚îÄ‚îÄ graph/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ state.py             # State management with TypedDict and Pydantic models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ workflow.py          # LangGraph workflow orchestration with routing logic
‚îÇ   ‚îî‚îÄ‚îÄ prompts/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ templates.py         # Agent prompts, personalities, and resume context
‚îú‚îÄ‚îÄ app.py                       # Streamlit web application (Main UI) ‚≠ê
‚îú‚îÄ‚îÄ main.py                      # CLI application (Alternative interface)
‚îú‚îÄ‚îÄ azure_clients.py             # Azure OpenAI client initialization
‚îú‚îÄ‚îÄ test_agent.py                # Test script for individual agents
‚îú‚îÄ‚îÄ .env                         # Environment variables (not in git)
‚îú‚îÄ‚îÄ .gitignore                   # Git ignore patterns
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ STREAMLIT_README.md          # Streamlit-specific documentation
‚îî‚îÄ‚îÄ README.md                    # This file
```


```

**Features:**
- üé® Professional gradient UI design
- ÔøΩ Resume upload with automatic text extraction (PDF, DOCX, TXT)
- ÔøΩüìä Real-time progress tracking across all interview rounds
- ü§ñ AI-powered evaluation with detailed feedback
- üéØ Automatic scoring (0-100)
- üí™ Personalized strengths analysis
- üîß Constructive areas for improvement
- üí° Actionable suggestions for growth
- üìã Complete interview transcript
- üîÑ Automatic reload on code changes
- üìù Resume preview before interview starts
- üéØ Context-aware questions based on uploaded resume



### Interview Flow

1. **Welcome Screen** üëã
   - Enter your name
   - Select job role (Software Engineer, Data Scientist, Product Manager, etc.)
   - Choose experience level (Junior/Mid-Level/Senior)
   - **Upload Resume (Optional)** üìÑ
     - Supports PDF, DOCX, and TXT formats
     - Resume content is extracted and used for personalized questions
     - Preview extracted text before starting

2. **Technical Round** üíª (6 questions)
   - Alex, the Technical Interviewer, asks coding, system design, and problem-solving questions
   - Questions adapt to your experience level and resume content
   - Topics: algorithms, best practices, system architecture, technical skills

3. **HR Round** ü§ù (3 questions)
   - Olivia, the HR Manager, assesses cultural fit and soft skills
   - Focus on teamwork, communication, conflict resolution, and values
   - Questions may reference your resume experience

4. **Managerial Round** üëî (2 questions)
   - Rahul, the Hiring Manager, evaluates leadership and strategic thinking
   - Questions about decision-making, career vision, and growth potential
   - Tailored to your experience level and background

5. **AI Evaluation** üéØ
   - Comprehensive analysis of your entire interview performance
   - Detailed score (0-100) with clear breakdown
   - Specific strengths identified across all rounds
   - Constructive areas for improvement with context
   - Actionable suggestions for professional growth
   - Overall hiring recommendation based on performance

### Example Web Interface

Visit the app to experience:
- Beautiful gradient cards for each interview round
- Resume upload section with drag-and-drop support
- File format validation and automatic text extraction
- Resume content preview with character count
- Real-time question counters (e.g., "Question 3/6")
- Progress bar showing overall completion
- Professional results screen with color-coded scores
- Detailed feedback sections with icons and formatting
- Comprehensive logging for debugging and monitoring

## üèóÔ∏è Architecture

### LangGraph Workflow

The system uses LangGraph to orchestrate a multi-agent interview process with AI evaluation:

```mermaid
graph TD
    A[Start Interview] --> B[Technical Agent - Alex]
    B -->|6 Questions| C[Technical Round Complete]
    C --> D[HR Agent - Olivia]
    D -->|3 Questions| E[HR Round Complete]
    E --> F[Manager Agent - Rahul]
    F -->|2 Questions| G[Manager Round Complete]
    G --> H[Evaluation Agent]
    H -->|Analysis & Scoring| I[End - Display Results]
    
    style A fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style B fill:#667eea,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#f093fb,stroke:#333,stroke-width:2px,color:#fff
    style F fill:#4facfe,stroke:#333,stroke-width:2px,color:#fff
    style H fill:#11998e,stroke:#333,stroke-width:2px,color:#fff
    style I fill:#38ef7d,stroke:#333,stroke-width:2px,color:#fff
```

**Workflow Details:**
- **Technical Agent (Alex)**: Asks 6 technical questions about coding, system design, and algorithms
- **HR Agent (Olivia)**: Asks 3 questions about cultural fit, soft skills, and teamwork
- **Manager Agent (Rahul)**: Asks 2 questions about leadership, strategy, and career vision
- **Evaluation Agent**: Analyzes entire interview and provides score, strengths, weaknesses, and suggestions
- **Results**: Comprehensive evaluation with actionable feedback

### State Management

### Agent Design

Each agent:
- Inherits from `BaseAgent`
- Has a unique personality defined in prompt templates
- Uses Azure OpenAI via centralized client (`azure_clients.py`)
- Maintains context from previous answers
- Adapts questions dynamically based on experience level
- Leverages resume content when available for personalized questions
- Generates natural, conversational questions without numbering
- Logs all question generation and state transitions

## üõ†Ô∏è Technical Components

### Logging System
- **File Handler**: Writes detailed DEBUG logs to `logs/ai_interviewer_TIMESTAMP.log`
- **Console Handler**: Displays INFO level logs to terminal
- **Module-Level Loggers**: Each module has its own logger for granular tracking
- **HTTP Noise Reduction**: Suppresses verbose logs from httpx, httpcore, openai libraries
- **Automatic Directory Creation**: `logs/` directory created automatically if needed

### Document Processing
- **PDF Support**: PyPDF2 for extracting text from PDF resumes
- **DOCX Support**: python-docx for parsing Word documents
- **TXT Support**: Native text file reading
- **Character Limit**: Resume text limited to first 2000 characters for context injection
- **Error Handling**: Graceful fallback if resume parsing fails




## üë§ Author

**Debdoot**
- GitHub: [@debdoot9804](https://github.com/debdoot9804)

## üôè Acknowledgments

- Built with [LangGraph](https://github.com/langchain-ai/langgraph) for multi-agent orchestration
- Powered by [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service)
- Uses [LangChain](https://github.com/langchain-ai/langchain) framework
- UI built with [Streamlit](https://streamlit.io/)
- Colorful terminal output with [Colorama](https://github.com/tartley/colorama)
